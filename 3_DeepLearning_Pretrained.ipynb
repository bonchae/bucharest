{"cells":[{"cell_type":"markdown","id":"837e8596-8652-47b3-9c12-66fbfb94b010","metadata":{"id":"837e8596-8652-47b3-9c12-66fbfb94b010"},"source":["# Pre-trained Deep Learning Models!"]},{"cell_type":"markdown","id":"GPYqFTGYx-Gz","metadata":{"id":"GPYqFTGYx-Gz"},"source":["## What are pre-trained Models?"]},{"cell_type":"markdown","id":"Sx7prLyIZRH1","metadata":{"id":"Sx7prLyIZRH1"},"source":["- The landscape of deep learning (DL) has evolved dramatically over the past decade.\n","- **Transitioning from traditional feed-forward neural networks** focused on training task-specific models **from scratch** to **transformer** architectures that leverage massive **pre-trained models** capable of adapting to diverse applications through fine-tuning and prompting.\n","- This **paradigm shift** has **revolutionized performance across domains while fundamentally changing how AI systems are developed and deployed**."]},{"cell_type":"markdown","id":"USEokzE6JZE-","metadata":{"id":"USEokzE6JZE-"},"source":["\u003cimg src=\"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/cecbccba-6358-476e-9fd8-e2807de9f220/Frame_118.png?t=1693044751\" width=500\u003e\n","\n","Founded in 2016\n","\n","Thousands of models (e.g., BERT, ChatGPT) you can use **without training from scratch**!\n","\n","[Go to Hugging Face](https://huggingface.co/) and explore [the pre-trained models available on the website](https://huggingface.co/models).\n","\n"]},{"cell_type":"markdown","id":"sgC_OzEa1xtJ","metadata":{"id":"sgC_OzEa1xtJ"},"source":["## Popularity Ranking of DL Architectures (as of 2025)\n","\n","| Rank | Architecture     | Popularity               | Primary Use Cases                                        |\n","|------|------------------|--------------------------|----------------------------------------------------------|\n","| 1️⃣   | Transformers      | ⭐⭐⭐⭐⭐ *(most popular)*    | LLMs, NLP, vision, audio, multimodal                     |\n","| 2️⃣   | CNNs              | ⭐⭐⭐⭐                     | Image classification, object detection, vision tasks     |\n","| 3️⃣   | GANs              | ⭐⭐⭐                      | Image generation, style transfer, data augmentation      |\n","| 4️⃣   | RNNs / LSTMs      | ⭐⭐                       | Legacy NLP, time series prediction, audio modeling       |\n"]},{"cell_type":"markdown","id":"c848d599-c263-4bbd-b6e3-8e75e0b53e29","metadata":{"id":"c848d599-c263-4bbd-b6e3-8e75e0b53e29"},"source":["# CNNs: Convolutional Neural Networks"]},{"cell_type":"markdown","id":"DMRbgW3q6rXD","metadata":{"id":"DMRbgW3q6rXD"},"source":["## Image classification, detection, vision"]},{"cell_type":"markdown","id":"bFgdAoISe2Kb","metadata":{"id":"bFgdAoISe2Kb"},"source":["\u003cimg src=\"https://hips.hearstapps.com/hmg-prod/images/pembroke-welsh-corgi-royalty-free-image-1726720011.jpg?crop=1.00xw:0.756xh;0,0.134xh\u0026resize=1024:\"\u003e"]},{"cell_type":"markdown","id":"yxnvWnK62Up1","metadata":{"id":"yxnvWnK62Up1"},"source":["### [ResNet-50 v1.5](https://huggingface.co/microsoft/resnet-50)\n","\n","\"ResNet (Residual Network) is a convolutional neural network. ResNet model pre-trained on ImageNet-1k at resolution 224x224.\"\n","\n","- 1,000 object categories (classes)\n","- 1.2 million training images\n","- 50,000 validation images\n"]},{"cell_type":"code","execution_count":null,"id":"slQA8BhF3OaD","metadata":{"id":"slQA8BhF3OaD"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from transformers import pipeline\n","import torch\n","\n","import textwrap # print output in multiple lines"]},{"cell_type":"code","execution_count":null,"id":"Ve2Dd0BXL4cD","metadata":{"id":"Ve2Dd0BXL4cD"},"outputs":[],"source":["# Check for CUDA (GPU)\n","device = 0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n","print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n","\n","# Load image classification pipeline with ResNet-50 (CNN)\n","classifier = pipeline(\n","    \"image-classification\",\n","    model=\"microsoft/resnet-50\",\n","    device=device,\n","    use_fast=True  # Use the fast image processor to avoid the warning\n",")\n","\n","# Classify the image\n","result = classifier(\"https://hips.hearstapps.com/hmg-prod/images/pembroke-welsh-corgi-royalty-free-image-1726720011.jpg\")\n","\n","for item in result:\n","    print(f\"Label: {item['label']}, Score: {item['score']:.4f}\")"]},{"cell_type":"markdown","id":"38RbhJfM8nI0","metadata":{"id":"38RbhJfM8nI0"},"source":["\u003cimg src=\"https://consumer-cms.petfinder.com/sites/default/files/images/content/Golden%20Retriever%201.jpg\"\u003e"]},{"cell_type":"code","execution_count":null,"id":"0afe0dcd-e736-4f70-ba51-c37a265f397a","metadata":{"id":"0afe0dcd-e736-4f70-ba51-c37a265f397a"},"outputs":[],"source":["# creat labels for the above dog\n","\n","result = classifier(\"https://hips.hearstapps.com/hmg-prod/images/pembroke-welsh-corgi-royalty-free-image-1726720011.jpg\")\n","\n","for item in result:\n","    print(f\"Label: {item['label']}, Score: {item['score']:.4f}\")"]},{"cell_type":"markdown","id":"82FkBDGlMRj8","metadata":{"id":"82FkBDGlMRj8"},"source":["\u003cimg src=\"https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\"\u003e"]},{"cell_type":"code","execution_count":null,"id":"5O1OJevJiZeC","metadata":{"id":"5O1OJevJiZeC"},"outputs":[],"source":["# try the above image. what labels do you expect?\n","# https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"bFHf5o_3aK8v","metadata":{"id":"bFHf5o_3aK8v"},"source":["Try a different method (Transformer) ...\n","\n","### [CLIP model](https://huggingface.co/docs/transformers/en/model_doc/clip)\n","\n","\"CLIP is a is a multimodal vision and language model motivated by **overcoming the fixed number of object categories** when training a computer vision model. CLIP learns about images directly from raw text by jointly training on 400M (image, text) pairs. Pretraining on this scale enables **zero-shot transfer** to downstream tasks.\" Developed by the OpenAI organization.\n","\n","This is a **transformer**-based model."]},{"cell_type":"code","execution_count":null,"id":"H6pyrvbP5Xol","metadata":{"id":"H6pyrvbP5Xol"},"outputs":[],"source":["from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","import torch, requests\n","\n","# Load model \u0026 processor\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"openai/clip-vit-base-patch32\"\n","model = CLIPModel.from_pretrained(model_id).to(device)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","\n","# Image and candidate captions\n","image = Image.open(requests.get(\n","    \"https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\",\n","    stream=True).raw)\n","\n","texts = [\"a photo of BTS\",\n","         \"a photo of a dog\",\n","         \"a photo of a band\",\n","         \"a group of men\"]\n","\n","# Predict\n","inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n","probs = model(**inputs).logits_per_image.softmax(dim=1)[0]\n","\n","# Display results\n","print(\"\\n CLIP Similarity Scores:\")\n","for text, p in zip(texts, probs):\n","    print(f\"{text:\u003c25} -\u003e {p:.4f}\")\n"]},{"cell_type":"markdown","id":"J5YeWtcbOEL9","metadata":{"id":"J5YeWtcbOEL9"},"source":["\u003cimg src=\"https://s.yimg.com/ny/api/res/1.2/UrUx_Vbbk413oGzvWSklPA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTI0MDA7aD0xNjAw/https://media.zenfs.com/en/parade_250/0b28a903a2ed548d063f996165786cd4\"\u003e"]},{"cell_type":"code","execution_count":null,"id":"vx4YGsvjN7ZO","metadata":{"id":"vx4YGsvjN7ZO"},"outputs":[],"source":["# Try the above image. Who is this person?\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"c06bbeb7-53c8-459f-a1a3-29277e4549c3","metadata":{"id":"c06bbeb7-53c8-459f-a1a3-29277e4549c3"},"source":["# Transformers: LLMs / Attention-Based Models"]},{"cell_type":"markdown","id":"lprgxb8MPEIs","metadata":{"id":"lprgxb8MPEIs"},"source":["## Sentiment analysis\n","\n","\n","[distilbert/distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)"]},{"cell_type":"code","execution_count":null,"id":"49617b85-3c7e-4316-99ec-697e8ee794ca","metadata":{"id":"49617b85-3c7e-4316-99ec-697e8ee794ca"},"outputs":[],"source":["from transformers import pipeline\n","\n","sentiment = pipeline(\"sentiment-analysis\",\n","                     model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n","\n","print(sentiment(\"This product is awful and I want a refund.\"))\n","print(sentiment(\"Average experience, nothing special.\"))\n","print(sentiment(\"Absolutely love the new update!\"))"]},{"cell_type":"code","execution_count":null,"id":"l-wuGCCbOd35","metadata":{"id":"l-wuGCCbOd35"},"outputs":[],"source":["# Try a different sentence for sentiment analysis\n","\n","\n"]},{"cell_type":"markdown","id":"zV2MugCEQPqt","metadata":{"id":"zV2MugCEQPqt"},"source":["## Emotion Detection\n","Use case: Go beyond \"positive/negative\" — detect emotions like joy, anger, sadness"]},{"cell_type":"markdown","id":"1YuISP_xQnop","metadata":{"id":"1YuISP_xQnop"},"source":["\u003cimg src=\"https://webflow-amber-prod.gumlet.io/620e4101b2ce12a1a6bff0e8/66ab6846124b51c486c24b3e_640f1bb03074900cbf0f28f3_What-are-the-Ivy-League-schools.webp\"\u003e"]},{"cell_type":"code","execution_count":null,"id":"9lzBEReFQRVd","metadata":{"id":"9lzBEReFQRVd"},"outputs":[],"source":["emotion = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=None)\n","result = emotion(\"I can't believe I got in! I'm so happy and feel very grateful.\")\n","\n","for row in result:\n","    for item in row:\n","        print(f\"{item['label']:\u003c10} -\u003e {item['score']:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"GhHvdccdjwFW","metadata":{"id":"GhHvdccdjwFW"},"outputs":[],"source":["# Try another expression for Emotion Detection\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"aqAhXM7SOvsl","metadata":{"id":"aqAhXM7SOvsl"},"source":["## Text Generation / Chatbots\n","\n","Use case: Writing, storytelling, character dialogue"]},{"cell_type":"code","execution_count":null,"id":"XuVc0FwP-HRg","metadata":{"id":"XuVc0FwP-HRg"},"outputs":[],"source":["from transformers import pipeline\n","import torch\n","\n","# Use GPU if available\n","device = 0 if torch.cuda.is_available() else -1\n","\n","# Create text generation pipeline (defaults to GPT-2)\n","generator = pipeline(\"text-generation\", model=\"gpt2\", device=device)\n","\n","# Generate text\n","output = generator(\n","    \"Once upon a time in Bucharest ...\",\n","    max_length=500,\n","    truncation=True,\n","    pad_token_id=generator.tokenizer.eos_token_id\n",")\n","\n","wrapped = textwrap.fill(output[0][\"generated_text\"], width=80)\n","print(\"\\n Generated Text:\\n\" + \"-\"*80)\n","print(wrapped)\n","print(\"-\"*80)\n"]},{"cell_type":"code","execution_count":null,"id":"n27HwJA2naxq","metadata":{"id":"n27HwJA2naxq"},"outputs":[],"source":["# Try to generate different texts\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"MIEX9wr5PyCl","metadata":{"id":"MIEX9wr5PyCl"},"source":["## Image Captioning\n","Use case: Describe an image using natural language using the [the BLIP model](https://huggingface.co/docs/transformers/en/model_doc/blip)\n","\n","BLIP is a model that is able to perform various multi-modal tasks including:\n","\n","- Visual Question Answering\n","- Image-Text retrieval (Image-text matching)\n","- Image Captioning"]},{"cell_type":"markdown","id":"VY5PvmQm_4pT","metadata":{"id":"VY5PvmQm_4pT"},"source":["\u003cimg src=\"https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\"\u003e"]},{"cell_type":"code","execution_count":null,"id":"bCPmc328OuzM","metadata":{"id":"bCPmc328OuzM"},"outputs":[],"source":["from transformers import BlipProcessor, BlipForConditionalGeneration\n","from PIL import Image\n","import requests\n","\n","image = Image.open(requests.get(\"https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\", stream=True).raw)\n","\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","\n","inputs = processor(image, return_tensors=\"pt\")\n","out = model.generate(**inputs)\n","print(processor.decode(out[0], skip_special_tokens=True))\n"]},{"cell_type":"markdown","id":"O2Y7N-n1CN8h","metadata":{"id":"O2Y7N-n1CN8h"},"source":["btt? not BTS?\n","\n","Oops! That looks like a hallucination from the model.\n","\n","Advanced models (e.g., [BLIP 2](https://huggingface.co/docs/transformers/en/model_doc/blip-2)) are more accurate."]},{"cell_type":"markdown","id":"1v-joSSBSOPN","metadata":{"id":"1v-joSSBSOPN"},"source":["## Audio Transcription (Speech-to-Text)\n","Use case: Convert speech into text using Whisper.\n","\n","[openai/whisper-small](https://huggingface.co/openai/whisper-small) is a pre-trained model for automatic speech recognition (ASR) and speech translation."]},{"cell_type":"code","execution_count":null,"id":"hlkNTWgYOu2L","metadata":{"id":"hlkNTWgYOu2L"},"outputs":[],"source":["from IPython.display import Audio, display\n","\n","# Direct link to the audio file\n","audio_url = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n","\n","# Embed audio player\n","display(Audio(audio_url))"]},{"cell_type":"code","execution_count":null,"id":"GlUnxvc3iZXy","metadata":{"id":"GlUnxvc3iZXy"},"outputs":[],"source":["asr = pipeline(\n","    \"automatic-speech-recognition\",\n","    model=\"openai/whisper-small\",\n","    generate_kwargs={\"task\": \"translate\", \"language\": \"en\"}\n",")\n","\n","output = asr(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n","\n","wrapped_text = textwrap.fill(output[\"text\"], width=80)\n","\n","print(\"Transcription:\\n\")\n","print(wrapped_text)"]},{"cell_type":"markdown","id":"TSwtV3TJS5Yu","metadata":{"id":"TSwtV3TJS5Yu"},"source":["## Translation"]},{"cell_type":"code","execution_count":null,"id":"gJQRCsIwS5wd","metadata":{"id":"gJQRCsIwS5wd"},"outputs":[],"source":["translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ro\")\n","text = \"I Love You All!\"\n","\n","translation = translator(text)[0]['translation_text']\n","print(translation)"]},{"cell_type":"markdown","id":"879f0270-68fe-44d9-ae31-48f8b366ba82","metadata":{"id":"879f0270-68fe-44d9-ae31-48f8b366ba82"},"source":["# GANs: Generative Adversarial Networks -- This is process heavy ... taking very long :(\n","\n","Image generations (DALL·E (OpenAI))"]},{"cell_type":"code","execution_count":null,"id":"oqItMKCNzMNa","metadata":{"id":"oqItMKCNzMNa"},"outputs":[],"source":["# install required packages for deep learning\n","#!pip install -q huggingface_hub[hf_xet]"]},{"cell_type":"markdown","id":"FWRbRSHM2-3l","metadata":{"id":"FWRbRSHM2-3l"},"source":["[Stable Diffusion 2.1 Version](https://huggingface.co/spaces/stabilityai/stable-diffusion) on Huggingface"]},{"cell_type":"code","execution_count":null,"id":"0JiHjMIUZgrY","metadata":{"colab":{"background_save":true},"id":"0JiHjMIUZgrY"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c029152b7b5940c1be4287b3f5d29956","version_major":2,"version_minor":0},"text/plain":["model_index.json:   0%|          | 0.00/541 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d54092596d994ed994494111a79b6e26","version_major":2,"version_minor":0},"text/plain":["Fetching 16 files:   0%|          | 0/16 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e4454aec68a439c882be9c2919bd3c2","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/342 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf442b95f3c347af961d597b938915e4","version_major":2,"version_minor":0},"text/plain":["scheduler_config-checkpoint.json:   0%|          | 0.00/209 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc5f7ff472cd47c49c864e447fc0cd88","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/4.56k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"250c714f860e49dfb7693264f0a1f581","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6d609d6b6174ba0a492fe6432ba7401","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/592 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0fafea8dd03f435eac92801f1936bdc4","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.22G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98ad1b2f45404b47809646007ee38c15","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/492M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f08bfae8da64a1abc9aa462c7a96b2b","version_major":2,"version_minor":0},"text/plain":["scheduler_config.json:   0%|          | 0.00/313 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b9b2d5513c84515a6993374a3f26a76","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/472 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6a34a5f4dce4c959c9535cba81e9d3c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/806 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3c70fab228648eb81028e7a1fa5813b","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.06M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1982704092d74f2b880d069ee8a34d6e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/743 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7597b1f369014dbeaca89d51dd5be7a9","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/551 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca2deab9bba54befb29e40c8173e439d","version_major":2,"version_minor":0},"text/plain":["diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1eccad8c3ea4aebb08574959531db3d","version_major":2,"version_minor":0},"text/plain":["diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bdffea059da14b69b112e37fed805f3d","version_major":2,"version_minor":0},"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Generating image... please wait ⏳\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0715bc6da09e45649a34578b8f8a6f80","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from diffusers import StableDiffusionPipeline\n","import torch\n","from PIL import Image\n","from IPython.display import display\n","\n","# Check if CUDA is available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","# Load the model (use full ID: CompVis/stable-diffusion-v1-4)\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    \"CompVis/stable-diffusion-v1-4\",\n","    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n","    low_cpu_mem_usage=True  # helps on CPU!\n",").to(device)\n","\n","# Prompt\n","prompt = \"A futuristic smart home device in minimal style product photography\"\n","\n","# Generate\n","print(\"Generating image... please wait ⏳\")\n","image = pipe(prompt).images[0]\n","\n","# Display\n","display(image)\n"]},{"cell_type":"code","execution_count":null,"id":"dcFMI01-iGdm","metadata":{"id":"dcFMI01-iGdm"},"outputs":[],"source":["# Try another prompt (e.g., A futuristic intelligent vehicle)\n","\n","\n","\n","\n","\n","\n"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}